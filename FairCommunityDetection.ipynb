{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProtectedAttributesTwitch(G, df):\n",
    "    protected_attributes = dict(zip(df[\"numeric_id\"], df[\"protected\"]))\n",
    "    newG = nx.set_node_attributes(G, protected_attributes, \"protected\")\n",
    "    return newG\n",
    "\n",
    "\n",
    "def getProtectedAttributesDeezer(G, df):\n",
    "    protected_attributes = dict(zip(df[\"id\"], df[\"target\"]))\n",
    "    newG = nx.set_node_attributes(G, protected_attributes, \"protected\")\n",
    "    return newG\n",
    "\n",
    "\n",
    "def getProtectedAttributesPokec(G, df):\n",
    "    protected_attributes = dict(zip(df[0], df[3]))\n",
    "    newG = nx.set_node_attributes(G, protected_attributes, \"protected\")\n",
    "    return newG\n",
    "\n",
    "\n",
    "def getprotectedAttributesDict_Facebook(featuresDF, featureNameDF, egoFeatDF):\n",
    "    gender_index = featureNameDF.index[featureNameDF[1] == \"gender;anonymized\"].to_list()[\n",
    "        0]\n",
    "    featuresDF = featuresDF[[0, gender_index + 1]]\n",
    "    egoFeatDF = egoFeatDF[[0, gender_index + 1]]\n",
    "    featuresDict = dict(zip(featuresDF[0], featuresDF[gender_index + 1]))\n",
    "    egoFeatDict = dict(zip(egoFeatDF[0], egoFeatDF[gender_index + 1]))\n",
    "    featuresDict.update(egoFeatDict)\n",
    "    return featuresDict\n",
    "\n",
    "\n",
    "def getprotectedAttributesDict_GPlus(featuresDF, gender_index, egoFeatDF):\n",
    "    # gender_index = featureNameDF.index[featureNameDF[1] == \"gender:1\"].to_list()[0]\n",
    "    featuresDF = featuresDF[[0, gender_index + 1]]\n",
    "    egoFeatDF = egoFeatDF[[0, gender_index + 1]]\n",
    "    featuresDict = dict(zip(featuresDF[0], featuresDF[gender_index + 1]))\n",
    "    egoFeatDict = dict(zip(egoFeatDF[0], egoFeatDF[gender_index + 1]))\n",
    "    featuresDict.update(egoFeatDict)\n",
    "    return featuresDict\n",
    "\n",
    "\n",
    "def getEgoFeats(path):\n",
    "    featFiles = []\n",
    "    featNameFiles = []\n",
    "    egoFeatFiles = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".feat\"):\n",
    "            featFiles.append(file)\n",
    "        if file.endswith(\".featnames\"):\n",
    "            featNameFiles.append(file)\n",
    "        if file.endswith(\".egofeat\"):\n",
    "            egoFeatFiles.append(file)\n",
    "    return featFiles, featNameFiles, egoFeatFiles\n",
    "\n",
    "\n",
    "def getProtectedAttributesFacebook(G):\n",
    "    node_gender_dict = {}\n",
    "    localpath = \"Datasets/facebook/facebook/\"\n",
    "    fbFeatFiles, fbFeatNameFiles, fbEgoFeatFiles = getEgoFeats(localpath)\n",
    "    for index in range(len(fbFeatFiles)):\n",
    "        localFeaturesDF = pd.read_csv(\n",
    "            localpath + fbFeatFiles[index], sep=\" \", header=None)\n",
    "        localFeatureNamesDf = pd.read_csv(\n",
    "            localpath + fbFeatNameFiles[index], sep=\" \", header=None)\n",
    "        localEgoFeatDf = pd.read_csv(\n",
    "            localpath + fbEgoFeatFiles[index], sep=\" \", header=None)\n",
    "        protectedAttrDict = getprotectedAttributesDict_Facebook(\n",
    "            localFeaturesDF, localFeatureNamesDf, localEgoFeatDf)\n",
    "        node_gender_dict.update(protectedAttrDict)\n",
    "\n",
    "    newG = nx.set_node_attributes(G, node_gender_dict, \"protected\")\n",
    "    return newG\n",
    "\n",
    "\n",
    "def getProtectedAttributesGPlus(G):\n",
    "    node_gender_dict = {}\n",
    "    localpath = \"Datasets/gplus/\"\n",
    "    gplusFeatFiles, gplusFeatNameFiles, gplusEgoFeatFiles = getEgoFeats(\n",
    "        localpath)\n",
    "    for index in range(len(gplusFeatFiles)):\n",
    "        try:\n",
    "            localFeaturesDF = pd.read_csv(\n",
    "                localpath + gplusFeatFiles[index], sep=\" \", header=None)\n",
    "        # localFeatureNamesDf = pd.read_csv(localpath + gplusFeatNameFiles[index], sep=\" \", header=None)\n",
    "            localEgoFeatDf = pd.read_csv(\n",
    "                localpath + gplusEgoFeatFiles[index], sep=\" \", header=None)\n",
    "            protectedAttrDict = getprotectedAttributesDict_GPlus(\n",
    "                localFeaturesDF, 0, localEgoFeatDf)\n",
    "            node_gender_dict.update(protectedAttrDict)\n",
    "        except:\n",
    "            print(gplusEgoFeatFiles[index])\n",
    "\n",
    "    newG = nx.set_node_attributes(G, node_gender_dict, \"protected\")\n",
    "    return newG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_community_attribute_Counter(communitiesList):\n",
    "    protectedAttributeCountDict = {}\n",
    "    communityCount = 0\n",
    "    for i in range(len(communitiesList)):\n",
    "        protectedAttributeCountDict[\"Community_{}\".format(communityCount)] = {\n",
    "            0: 0, 1: 0}\n",
    "        communityCount += 1\n",
    "    return protectedAttributeCountDict\n",
    "\n",
    "\n",
    "def count_protected_attributes_frequency(G, communitiesList, protectedAttributeCountDict, attribute):\n",
    "    communityCount = 0\n",
    "    for community in communitiesList:\n",
    "        for node in community:\n",
    "            try:\n",
    "                if G.nodes()[node][attribute] == 0:\n",
    "                    protectedAttributeCountDict[\"Community_{}\".format(\n",
    "                        communityCount)][0] += 1\n",
    "                else:\n",
    "                    protectedAttributeCountDict[\"Community_{}\".format(\n",
    "                        communityCount)][1] += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        communityCount += 1\n",
    "    return protectedAttributeCountDict\n",
    "\n",
    "\n",
    "def calculate_community_balance(protectedAttributeCountDict):\n",
    "    for x in protectedAttributeCountDict:\n",
    "        red = protectedAttributeCountDict[x][0]\n",
    "        blue = protectedAttributeCountDict[x][1]\n",
    "        if red == 0 and blue == 0:\n",
    "            pass\n",
    "        elif red >= blue:\n",
    "            balance = blue/red\n",
    "            protectedAttributeCountDict[x][\"balance\"] = balance\n",
    "        else:\n",
    "            balance = red/blue\n",
    "            protectedAttributeCountDict[x][\"balance\"] = balance\n",
    "    return protectedAttributeCountDict\n",
    "\n",
    "\n",
    "def calculate_Fairness(G, communitiesList, attribute):\n",
    "\n",
    "    protectedAttributeCountDict = initialize_community_attribute_Counter(\n",
    "        communitiesList)\n",
    "\n",
    "    protectedAttributeCountDict = count_protected_attributes_frequency(\n",
    "        G, communitiesList, protectedAttributeCountDict, attribute)\n",
    "\n",
    "    protectedAttributeCountDict = calculate_community_balance(\n",
    "        protectedAttributeCountDict)\n",
    "\n",
    "    return protectedAttributeCountDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countInCommunityDegree(G, communities):\n",
    "    communitiesDegree = {}\n",
    "    # communityCount = 0\n",
    "    for index, community in enumerate(communities):\n",
    "        communityDegree = {}\n",
    "        communitySubgraph = G.subgraph(community)\n",
    "        for node in communitySubgraph.nodes():\n",
    "            communityDegree[node] = communitySubgraph.degree(node)\n",
    "        communitiesDegree[\"Community_{}\".format(index)] = communityDegree\n",
    "    return communitiesDegree\n",
    "\n",
    "\n",
    "def getLowInCommunityDegreeNodes(communitiesNodeDegrees):\n",
    "    lowInCommunityDegreeNodes = {}\n",
    "    for (communityName, community) in communitiesNodeDegrees.items():\n",
    "        percent = len(community.keys()) * 40 / 100\n",
    "        lowDegreeNodes = dict(\n",
    "            filter(lambda elem: elem[1] < percent, community.items()))\n",
    "        lowDegreeNodes = dict(\n",
    "            sorted(lowDegreeNodes.items(), key=lambda item: item[1]))\n",
    "        lowInCommunityDegreeNodes[communityName] = lowDegreeNodes\n",
    "    return lowInCommunityDegreeNodes\n",
    "\n",
    "\n",
    "def getLowInCommunityDegreeNodesProtectedAttribute(graph, lowInCommunityDegreeNodes, protectedAttributeLabel):\n",
    "    lowInCommunityDegreeNodesProtectedAttribute = []\n",
    "    for (node, degree) in lowInCommunityDegreeNodes.items():\n",
    "        try:\n",
    "            if graph.nodes()[node][\"protected\"] == protectedAttributeLabel:\n",
    "                lowInCommunityDegreeNodesProtectedAttribute.append(node)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return lowInCommunityDegreeNodesProtectedAttribute\n",
    "\n",
    "\n",
    "def calculate_global_fairness(fairnessDictionary):\n",
    "    zeroCounts = 0 \n",
    "    oneCounts = 0\n",
    "    for communityInfo in fairnessDictionary.values():\n",
    "        zeroCounts += communityInfo[0]\n",
    "        oneCounts += communityInfo[1]\n",
    "    try:\n",
    "        if zeroCounts > oneCounts:\n",
    "            return oneCounts / zeroCounts\n",
    "        else:\n",
    "            return zeroCounts / oneCounts\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    # return min(zeroCounts/oneCounts, oneCounts/zeroCounts)\n",
    "\n",
    "\n",
    "def getCommunitiesWithLowFairness(fairnessDictionary, threshold):\n",
    "    lowFairnessCommunities = []\n",
    "    for (communityName, communityInfo) in fairnessDictionary.items():\n",
    "        if communityInfo[\"balance\"] < threshold:\n",
    "            lowFairnessCommunities.append(communityName)\n",
    "    return lowFairnessCommunities\n",
    "\n",
    "\n",
    "def seperateLowFairnessCommunities(lowFairnesCommunities, fairnessDictionary):\n",
    "    zeroCommunities = []\n",
    "    oneCommunities = []\n",
    "\n",
    "    for community in lowFairnesCommunities:\n",
    "        if fairnessDictionary[community][0] > fairnessDictionary[community][1]:\n",
    "            zeroCommunities.append(community)\n",
    "        else:\n",
    "            oneCommunities.append(community)\n",
    "    return zeroCommunities, oneCommunities\n",
    "\n",
    "\n",
    "def updateFairnesDictionary(fairnessDictionary, startCommunity, destinationCommunity, startCommunityType):\n",
    "    fairnessDictionary[startCommunity][startCommunityType] -= 1\n",
    "    fairnessDictionary[destinationCommunity][startCommunityType] += 1\n",
    "    \n",
    "    if fairnessDictionary[startCommunity][0] > fairnessDictionary[startCommunity][1]:\n",
    "        newStartFairness = fairnessDictionary[startCommunity][1]/fairnessDictionary[startCommunity][0]\n",
    "    else:\n",
    "        newStartFairness = fairnessDictionary[startCommunity][0]/fairnessDictionary[startCommunity][1]\n",
    "\n",
    "    if fairnessDictionary[destinationCommunity][0] > fairnessDictionary[destinationCommunity][1]:\n",
    "        newDestinationFairness = fairnessDictionary[destinationCommunity][1]/fairnessDictionary[destinationCommunity][0]\n",
    "    else:\n",
    "        newDestinationFairness = fairnessDictionary[destinationCommunity][0]/fairnessDictionary[destinationCommunity][1]\n",
    "    \n",
    "    # newStartFairness = min((fairnessDictionary[startCommunity][0]/fairnessDictionary[startCommunity][1]),\n",
    "    #                        fairnessDictionary[startCommunity][1]/fairnessDictionary[startCommunity][0])\n",
    "    # newDestinationFairness = min((fairnessDictionary[destinationCommunity][0]/fairnessDictionary[destinationCommunity][1]),\n",
    "    #                              fairnessDictionary[destinationCommunity][1]/fairnessDictionary[destinationCommunity][0])\n",
    "    fairnessDictionary[startCommunity][\"balance\"] = newStartFairness\n",
    "    fairnessDictionary[destinationCommunity][\"balance\"] = newDestinationFairness\n",
    "\n",
    "    # print(\"New fairness for {}: {}\".format(startCommunity, newStartFairness))\n",
    "    # print(\"New fairness for {}: {}\".format(\n",
    "    #     destinationCommunity, newDestinationFairness))\n",
    "\n",
    "    return fairnessDictionary, newStartFairness, newDestinationFairness\n",
    "\n",
    "\n",
    "def sendNodes(startCommunityName, startCommunityNodes, destinationCommunities, fairnessDictionary, globalFairness, startCommunityType):\n",
    "    destinationCommunitiesNameList = destinationCommunities\n",
    "    destinationCounter = 0\n",
    "\n",
    "    startCommunityCurrentFairness = fairnessDictionary[startCommunityName][\"balance\"]\n",
    "    destinationCommunityCurrentFairness = fairnessDictionary[\n",
    "        destinationCommunitiesNameList[destinationCounter]][\"balance\"]\n",
    "\n",
    "\n",
    "    while (startCommunityCurrentFairness < globalFairness and len(startCommunityNodes) > 0 and destinationCounter < len(destinationCommunitiesNameList)):\n",
    "        # print(\"Destination Community: {}\".format(destinationCommunitiesNameList[destinationCounter]))\n",
    "        # print(\"Destination Community Current Fairness: {}\".format(destinationCommunityCurrentFairness))\n",
    "\n",
    "        if(destinationCommunityCurrentFairness >= globalFairness):\n",
    "            destinationCounter += 1\n",
    "            if(destinationCounter < len(destinationCommunitiesNameList)):\n",
    "                destinationCommunityCurrentFairness = fairnessDictionary[\n",
    "                    destinationCommunitiesNameList[destinationCounter]][\"balance\"]\n",
    "        else:\n",
    "            startCommunityNodes.pop(0)  # dont care where it goes for now\n",
    "            # Update stats and fairness\n",
    "            fairnessDictionary, startCommunityCurrentFairness, destinationCommunityCurrentFairness = updateFairnesDictionary(fairnessDictionary, startCommunityName,\n",
    "                                                                                                                             destinationCommunitiesNameList[destinationCounter], startCommunityType)\n",
    "\n",
    "    return fairnessDictionary\n",
    "\n",
    "\n",
    "def fixCommunityFairness(graph, fairnessDictionary, lowInCommunityDegreeNodes):\n",
    "\n",
    "    newFairnessDictionary = fairnessDictionary.copy()\n",
    "    zeroCommunitiesDict = {}\n",
    "    oneCommunitiesDict = {}\n",
    "\n",
    "    global_community_fairness = calculate_global_fairness(newFairnessDictionary)\n",
    "    lowFairnessCommunities = getCommunitiesWithLowFairness(\n",
    "        newFairnessDictionary, global_community_fairness)\n",
    "    zeroCommunities, oneCommunities = seperateLowFairnessCommunities(\n",
    "        lowFairnessCommunities, newFairnessDictionary)\n",
    "\n",
    "    # print(len(zeroCommunities))\n",
    "    # print(len(oneCommunities))\n",
    "    if len(zeroCommunities) == 0 or len(oneCommunities) == 0:\n",
    "        print(\"Fairness cannot  be fixed\")\n",
    "        return fairnessDictionary\n",
    "\n",
    "    for community in zeroCommunities:\n",
    "        nodes = getLowInCommunityDegreeNodesProtectedAttribute(\n",
    "            graph, lowInCommunityDegreeNodes[community], 0)\n",
    "        zeroCommunitiesDict[community] = nodes\n",
    "        # get the nodes that want to leave from each community\n",
    "\n",
    "    for community in oneCommunities:\n",
    "        nodes = getLowInCommunityDegreeNodesProtectedAttribute(\n",
    "            graph, lowInCommunityDegreeNodes[community], 1)\n",
    "        oneCommunitiesDict[community] = nodes\n",
    "\n",
    "    for community in zeroCommunities:\n",
    "        # print(\"Test for {}\".format(community))\n",
    "        # print()\n",
    "        newFairnessDictionary = sendNodes(community, zeroCommunitiesDict[community],\n",
    "                                       oneCommunities, fairnessDictionary, global_community_fairness, 0)\n",
    "    for community in oneCommunities:\n",
    "        # print(\"test for {}\".format(community))\n",
    "        newFairnessDictionary = sendNodes(community, oneCommunitiesDict[community],\n",
    "                                        zeroCommunities, fairnessDictionary, global_community_fairness, 1)\n",
    "\n",
    "    return newFairnessDictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Communities_To_Fairness_CSV(paths):\n",
    "    for path in paths:\n",
    "        communityDF = pd.read_csv(\"comE_communities/communities/{}\".format(path), sep=\"\\t\", header=None)\n",
    "        communityListDf = communityDF.groupby(1).aggregate(lambda x: list(x))\n",
    "        communitiesList = communityListDf[0].to_list()\n",
    "        if path.startswith(\"twitch\"):\n",
    "            protected_count = calculate_Fairness(twitchGamersCCGraph, communitiesList, \"protected\")\n",
    "            testdf = pd.DataFrame(protected_count)\n",
    "            testdf = testdf.T\n",
    "            testdf.to_csv(\"comE_communities/output/{}_fairness.csv\".format(path))\n",
    "        elif path.startswith(\"gplus\"):\n",
    "            protected_count = calculate_Fairness(gplus_graph, communitiesList, \"protected\")\n",
    "            testdf = pd.DataFrame(protected_count)\n",
    "            testdf = testdf.T\n",
    "            testdf.to_csv(\"comE_communities/output/{}_fairness.csv\".format(path))\n",
    "        elif path.startswith(\"facebook\"):\n",
    "            protected_count = calculate_Fairness(facebookLargestCCGraph, communitiesList, \"protected\")\n",
    "            testdf = pd.DataFrame(protected_count)\n",
    "            testdf = testdf.T\n",
    "            testdf.to_csv(\"comE_communities/output/{}_fairness.csv\".format(path))\n",
    "        elif path.startswith(\"deezer\"):\n",
    "            protected_count = calculate_Fairness(deezerLargestCCGraph, communitiesList, \"protected\")\n",
    "            testdf = pd.DataFrame(protected_count)\n",
    "            testdf = testdf.T\n",
    "            testdf.to_csv(\"comE_communities/output/{}_fairness.csv\".format(path))\n",
    "        elif path.startswith(\"pokec\"):\n",
    "            protected_count = calculate_Fairness(pokec_test, communitiesList, \"protected\")\n",
    "            testdf = pd.DataFrame(protected_count)\n",
    "            testdf = testdf.T\n",
    "            testdf.to_csv(\"fairness_CSV/{}_fairness.csv\".format(path))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateNewFairnessPipeline(paths):\n",
    "    for path in paths:\n",
    "        communityDF = pd.read_csv(\"communities/{}\".format(path), sep=\"\\t\", header=None)\n",
    "        communityListDf = communityDF.groupby(1).aggregate(lambda x: list(x))\n",
    "        communitiesList = communityListDf[0].to_list()\n",
    "        if path.startswith(\"twitch\"):\n",
    "            oldFairness = calculate_Fairness(twitchGamersCCGraph, communitiesList, \"protected\")\n",
    "            inCommunityDegreeDict = countInCommunityDegree(twitchGamersCCGraph, communitiesList)\n",
    "            LowDegreeNodes = getLowInCommunityDegreeNodes(inCommunityDegreeDict)\n",
    "            newFairness = fixCommunityFairness(twitchGamersCCGraph, oldFairness, LowDegreeNodes)\n",
    "\n",
    "            oldFairnessdf = pd.DataFrame(oldFairness)\n",
    "            oldFairnessdf = oldFairnessdf.T\n",
    "            print(\"Old Fairness\")\n",
    "            average_fairness_local(oldFairnessdf, path)\n",
    "            newFairnessdf = pd.DataFrame(newFairness)\n",
    "            newFairnessdf = newFairnessdf.T\n",
    "            print(\"New Fairness\")\n",
    "            average_fairness_local(newFairnessdf, path)\n",
    "            print(\"======================================================================\")\n",
    "            \n",
    "        elif path.startswith(\"gplus\"):\n",
    "            oldFairness = calculate_Fairness(gplusCCGraph, communitiesList, \"protected\")\n",
    "            inCommunityDegreeDict = countInCommunityDegree(gplusCCGraph, communitiesList)\n",
    "            LowDegreeNodes = getLowInCommunityDegreeNodes(inCommunityDegreeDict)\n",
    "            newFairness = fixCommunityFairness(gplusCCGraph, oldFairness, LowDegreeNodes)\n",
    "\n",
    "            oldFairnessdf = pd.DataFrame(oldFairness)\n",
    "            oldFairnessdf = oldFairnessdf.T\n",
    "            print(\"Old Fairness\")\n",
    "            average_fairness_local(oldFairnessdf, path)\n",
    "            newFairnessdf = pd.DataFrame(newFairness)\n",
    "            newFairnessdf = newFairnessdf.T\n",
    "            print(\"New Fairness\")\n",
    "            average_fairness_local(newFairnessdf, path)\n",
    "            print(\"======================================================================\")\n",
    "\n",
    "        elif path.startswith(\"facebook\"):\n",
    "            oldFairness = calculate_Fairness(facebookLargestCCGraph, communitiesList, \"protected\")\n",
    "            inCommunityDegreeDict = countInCommunityDegree(facebookLargestCCGraph, communitiesList)\n",
    "            LowDegreeNodes = getLowInCommunityDegreeNodes(inCommunityDegreeDict)\n",
    "            newFairness = fixCommunityFairness(facebookLargestCCGraph, oldFairness, LowDegreeNodes)\n",
    "\n",
    "            oldFairnessdf = pd.DataFrame(oldFairness)\n",
    "            oldFairnessdf = oldFairnessdf.T\n",
    "            print(\"Old Fairness\")\n",
    "            average_fairness_local(oldFairnessdf, path)\n",
    "            newFairnessdf = pd.DataFrame(newFairness)\n",
    "            newFairnessdf = newFairnessdf.T\n",
    "            print(\"New Fairness\")\n",
    "            average_fairness_local(newFairnessdf, path)\n",
    "            print(\"======================================================================\")\n",
    "        elif path.startswith(\"deezer\"):\n",
    "            oldFairness = calculate_Fairness(deezerLargestCCGraph, communitiesList, \"protected\")\n",
    "            inCommunityDegreeDict = countInCommunityDegree(deezerLargestCCGraph, communitiesList)\n",
    "            LowDegreeNodes = getLowInCommunityDegreeNodes(inCommunityDegreeDict)\n",
    "            newFairness = fixCommunityFairness(deezerLargestCCGraph, oldFairness, LowDegreeNodes)\n",
    "\n",
    "            oldFairnessdf = pd.DataFrame(oldFairness)\n",
    "            oldFairnessdf = oldFairnessdf.T\n",
    "            print(\"Old Fairness\")\n",
    "            average_fairness_local(oldFairnessdf, path)\n",
    "            newFairnessdf = pd.DataFrame(newFairness)\n",
    "            newFairnessdf = newFairnessdf.T\n",
    "            print(\"New Fairness\")\n",
    "            average_fairness_local(newFairnessdf, path)\n",
    "            print(\"======================================================================\")\n",
    "        elif path.startswith(\"pokec\"):\n",
    "            oldFairness = calculate_Fairness(pokecCCGraph, communitiesList, \"protected\")\n",
    "            inCommunityDegreeDict = countInCommunityDegree(pokecCCGraph, communitiesList)\n",
    "            LowDegreeNodes = getLowInCommunityDegreeNodes(inCommunityDegreeDict)\n",
    "            newFairness = fixCommunityFairness(pokecCCGraph, oldFairness, LowDegreeNodes)\n",
    "\n",
    "            oldFairnessdf = pd.DataFrame(oldFairness)\n",
    "            oldFairnessdf = oldFairnessdf.T\n",
    "            print(\"Old Fairness\")\n",
    "            average_fairness_local(oldFairnessdf, path)\n",
    "            newFairnessdf = pd.DataFrame(newFairness)\n",
    "            newFairnessdf = newFairnessdf.T\n",
    "            print(\"New Fairness\")\n",
    "            average_fairness_local(newFairnessdf, path)\n",
    "            print(\"======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_fairness_local(df, path):\n",
    "    # df = pd.read_csv(\"comE_communities/output/{}\".format(path), sep=\",\")\n",
    "    zeroCounts = df[0].sum()\n",
    "    oneCounts = df[1].sum()\n",
    "    if zeroCounts > oneCounts:\n",
    "        balance = oneCounts/zeroCounts\n",
    "    else:\n",
    "        balance = zeroCounts/oneCounts\n",
    "    # balance = min(zeroCounts/oneCounts, oneCounts/zeroCounts)\n",
    "    averageFairnes = df[[\"balance\"]].mean()\n",
    "    print(\"Average Fairness for {} is {}\".format(path, averageFairnes))\n",
    "    print(\"Global Fairness for {} is {}\".format(path, balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_fairness(paths):\n",
    "    for path in paths:\n",
    "        fairnessDf = pd.read_csv(\"comE_communities/output/{}\".format(path), sep=\",\")\n",
    "        zeroCounts = fairnessDf[\"0\"].sum()\n",
    "        oneCounts = fairnessDf[\"1\"].sum()\n",
    "        balance = min(zeroCounts/oneCounts, oneCounts/zeroCounts)\n",
    "        averageFairnes = fairnessDf[[\"balance\"]].mean()\n",
    "        print(\"Average Fairness for {} is {}\".format(path, averageFairnes))\n",
    "        print(\"Global Fairness for {} is {}\".format(path, balance))\n",
    "        print(\"=====================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(list):\n",
    "    return max(set(list), key=list.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Time 2mins\n",
    "# gplus_graph = nx.read_edgelist(\"{}gplus/gplus_combined.txt\".format(path), nodetype=str, delimiter=\" \", create_using=nx.DiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplus_graph = nx.read_edgelist(\"gplus/gplus_combined_undirected.txt\".format(path), nodetype=str, delimiter=\" \", create_using=nx.Graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getProtectedAttributesGPlus(gplus_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplusCC = max(nx.connected_components(gplus_graph), key=len)\n",
    "gplusCCGraph = gplus_graph.subgraph(gplusCC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitch Gamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitch gamers Graph\n",
    "twitchGamers_graph = nx.read_edgelist(\n",
    "    \"{}twitch_gamers/large_twitch_edges.csv\".format(path), nodetype=int, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitch gamers features\n",
    "twitchGamers_features = pd.read_csv(\n",
    "    \"{}twitch_gamers/large_twitch_features.csv\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getProtectedAttributesTwitch(twitchGamers_graph, twitchGamers_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitchGamersCC = max(nx.connected_components(twitchGamers_graph), key=len)\n",
    "twitchGamersCCGraph = twitchGamers_graph.subgraph(twitchGamersCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deezer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deezer europe graph\n",
    "deezer_graph = nx.read_edgelist(\n",
    "    \"{}deezer_europe/deezer_europe/deezer_europe_edges.csv\".format(path), nodetype=int, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Deezer genders\n",
    "deezer_gendersDf = pd.read_csv(\n",
    "    \"{}/deezer_europe/deezer_europe/deezer_europe_target.csv\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getProtectedAttributesDeezer(deezer_graph, deezer_gendersDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezerLargestCC = max(nx.connected_components(deezer_graph), key=len)\n",
    "deezerLargestCCGraph = deezer_graph.subgraph(deezerLargestCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution time 2m 26.1s\n",
    "# pokec_graph = nx.read_edgelist(\"{}/pokec/soc-pokec-relationships.txt\".format(path), nodetype=int, delimiter=\"\\t\", create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokec_test = nx.read_edgelist(\"pokec/soc-pokec-undirected.txt\",\n",
    "                              nodetype=int, delimiter=\"\\t\", create_using=nx.Graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokec_features = pd.read_csv(\n",
    "    \"{}/pokec/soc-pokec-profiles.txt\".format(path), delimiter=\"\\t\", header=None, usecols=[0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getProtectedAttributesPokec(pokec_test, pokec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokecCC = max(nx.connected_components(pokec_test), key=len)\n",
    "pokecCCGraph = pokec_test.subgraph(pokecCC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_graph_all = nx.read_edgelist(\n",
    "    \"{}facebook/facebook/facebook_combined.txt\".format(path), nodetype=int, delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getProtectedAttributesFacebook(facebook_graph_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebookLargestCC = max(nx.connected_components(facebook_graph_all), key=len)\n",
    "facebookLargestCCGraph = facebook_graph_all.subgraph(facebookLargestCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL TOGETHER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplus_graph = nx.read_edgelist(\"gplus/gplus_combined_undirected.txt\".format(path), nodetype=str, delimiter=\" \", create_using=nx.Graph())\n",
    "\n",
    "getProtectedAttributesGPlus(gplus_graph)\n",
    "gplusCC = max(nx.connected_components(gplus_graph), key=len)\n",
    "gplusCCGraph = gplus_graph.subgraph(gplusCC)\n",
    "\n",
    "twitchGamers_graph = nx.read_edgelist(\n",
    "    \"{}twitch_gamers/large_twitch_edges.csv\".format(path), nodetype=int, delimiter=\",\")\n",
    "\n",
    "twitchGamers_features = pd.read_csv(\n",
    "    \"{}twitch_gamers/large_twitch_features.csv\".format(path))\n",
    "\n",
    "\n",
    "getProtectedAttributesTwitch(twitchGamers_graph, twitchGamers_features)\n",
    "twitchGamersCC = max(nx.connected_components(twitchGamers_graph), key=len)\n",
    "twitchGamersCCGraph = twitchGamers_graph.subgraph(twitchGamersCC)\n",
    "\n",
    "deezer_graph = nx.read_edgelist(\n",
    "    \"{}deezer_europe/deezer_europe/deezer_europe_edges.csv\".format(path), nodetype=int, delimiter=\",\")\n",
    "\n",
    "deezer_gendersDf = pd.read_csv(\n",
    "    \"{}/deezer_europe/deezer_europe/deezer_europe_target.csv\".format(path))\n",
    "\n",
    "getProtectedAttributesDeezer(deezer_graph, deezer_gendersDf)\n",
    "\n",
    "deezerLargestCC = max(nx.connected_components(deezer_graph), key=len)\n",
    "deezerLargestCCGraph = deezer_graph.subgraph(deezerLargestCC)\n",
    "\n",
    "\n",
    "pokec_test = nx.read_edgelist(\"pokec/soc-pokec-undirected.txt\",\n",
    "                              nodetype=int, delimiter=\"\\t\", create_using=nx.Graph())\n",
    "\n",
    "pokec_features = pd.read_csv(\n",
    "    \"{}/pokec/soc-pokec-profiles.txt\".format(path), delimiter=\"\\t\", header=None, usecols=[0, 3])\n",
    "\n",
    "getProtectedAttributesPokec(pokec_test, pokec_features)\n",
    "\n",
    "pokecCC = max(nx.connected_components(pokec_test), key=len)\n",
    "pokecCCGraph = pokec_test.subgraph(pokecCC)\n",
    "\n",
    "facebook_graph_all = nx.read_edgelist(\n",
    "    \"{}facebook/facebook/facebook_combined.txt\".format(path), nodetype=int, delimiter=\" \")\n",
    "\n",
    "getProtectedAttributesFacebook(facebook_graph_all)\n",
    "\n",
    "facebookLargestCC = max(nx.connected_components(facebook_graph_all), key=len)\n",
    "facebookLargestCCGraph = facebook_graph_all.subgraph(facebookLargestCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communityDF = pd.read_csv(\"communities/gplusLouvain.txt\", sep=\"\\t\", header=None)\n",
    "communityListDf = communityDF.groupby(1).aggregate(lambda x: list(x))\n",
    "communitiesList = communityListDf[0].to_list()\n",
    "\n",
    "communitiesList\n",
    "\n",
    "oldFairness = calculate_Fairness(gplusCCGraph, communitiesList, \"protected\")\n",
    "df = pd.DataFrame(oldFairness)\n",
    "df = df.T\n",
    "print(\"Old Fairness\")\n",
    "print(df[[\"balance\"]].mean())\n",
    "\n",
    "\n",
    "# oldFairness\n",
    "inCommunityDegreeDict = countInCommunityDegree(gplusCCGraph, communitiesList)\n",
    "# inCommunityDegreeDict\n",
    "LowDegreeNodes = getLowInCommunityDegreeNodes(inCommunityDegreeDict)\n",
    "# LowDegreeNodes\n",
    "newFairness = fixCommunityFairness(gplusCCGraph, oldFairness, LowDegreeNodes)\n",
    "dfNew = pd.DataFrame(newFairness)\n",
    "dfNew = dfNew.T\n",
    "print(\"New Fairness\")\n",
    "print(dfNew[[\"balance\"]].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(oldFairness)\n",
    "df = df.T\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNew = pd.DataFrame(newFairness)\n",
    "dfNew = dfNew.T\n",
    "dfNew.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Plus Communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_communities_Gplus = nx.algorithms.community.louvain_communities(gplusCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Louvain algorithm to csv\n",
    "\n",
    "# community_counter = 0\n",
    "# with open(\"output/gplusLouvain.txt\", \"w\") as f:\n",
    "#     for community in louvain_communities_Gplus:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderCountGplus = {}\n",
    "genderCountGplus = calculate_Fairness(\n",
    "    gplusCCGraph, gplusCCGraph, 'gender')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_propagation_Gplus = nx.algorithms.community.label_propagation_communities(\n",
    "    gplusCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Label Propagation algorithm to csv\n",
    "# community_counter = 0\n",
    "# with open(\"output/gplusLabelPropagation.txt\", \"w\") as f:\n",
    "#     for community in label_propagation_Gplus:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitch Communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Εxecution time 5mins\n",
    "louvain_communities_twitch = nx.algorithms.community.louvain_communities(\n",
    "    twitchGamersCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Louvain algorithm to csv\n",
    "# community_counter = 0\n",
    "# with open(\"output/twitchLouvain.txt\", \"w\") as f:\n",
    "#     for community in louvain_communities_twitch:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_gender_count = {}\n",
    "louvain_gender_count = calculate_Fairness(\n",
    "    twitchGamers_graph, louvain_communities_twitch, \"protected\")\n",
    "louvain_gender_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12m execution time\n",
    "twitchGamersCCGraphComms = nx.algorithms.community.label_propagation_communities(\n",
    "    twitchGamersCCGraph)\n",
    "twitchLabelPropagationList = list(twitchGamersCCGraphComms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Label propagation algorithm to csv\n",
    "# community_counter = 0\n",
    "# with open(\"output/twitchLabelPropagation.txt\", \"w\") as f:\n",
    "#     for community in twitchLabelPropagationList:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderCountTwitch = {}\n",
    "genderCountTwitch = calculate_Fairness(\n",
    "    twitchGamers_graph, twitchLabelPropagationList, \"protected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deezer Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezerLabelPropagationComms = nx.algorithms.community.label_propagation_communities(\n",
    "    deezerLargestCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deezerLabelPropagationList = list(deezerLabelPropagationComms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Label propagation algorithm to csv\n",
    "# community_counter = 0\n",
    "# with open(\"output/deezerLabelPropagation.txt\", \"w\") as f:\n",
    "#     for community in deezerLabelPropagationList:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_communities_deezer = nx.community.louvain_communities(\n",
    "    deezerLargestCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Louvain algorithm to csv\n",
    "# community_counter = 0\n",
    "# with open(\"output/deezerLouvain.txt\", \"w\") as f:\n",
    "#     for community in louvain_communities_deezer:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pokec Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPropagationPokec = nx.algorithms.community.label_propagation_communities(\n",
    "    pokecCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Label propagation algorithm to csv\n",
    "# community_counter = 0\n",
    "# with open(\"output/pokecLabelPropagation.txt\", \"w\") as f:\n",
    "#     for community in labelPropagationPokec:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_communities_Pokec = nx.algorithms.community.louvain_communities(\n",
    "    pokecCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecagy Code for extracting the communities from the Louvain algorithm to csv\n",
    "# community_counter = 0\n",
    "# with open(\"output/pokecLouvain.txt\", \"w\") as f:\n",
    "#     for community in louvain_communities_Pokec:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebookComms = nx.community.label_propagation_communities(\n",
    "    facebookLargestCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebookLabelPropagationList = list(facebookComms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community_counter = 0\n",
    "# with open(\"output/facebookLabelPropagation.txt\", \"w\") as f:\n",
    "#     for community in facebookLabelPropagationList:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderCountFacebook = {}\n",
    "genderCountFacebook = calculate_Fairness(\n",
    "    facebook_graph_all, facebookLabelPropagationList, 'gender')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_global_fairness(genderCountFacebook))\n",
    "# print(calculate_global_fairness2(genderCountFacebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcommDegree = countInCommunityDegree(facebookLargestCCGraph, facebookLabelPropagationList)\n",
    "# testcommDegree\n",
    "\n",
    "\n",
    "facebookLowDegreeNodes = getLowInCommunityDegreeNodes(testcommDegree)\n",
    "# facebookLowDegreeNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderCountFacebook\n",
    "df = pd.DataFrame(genderCountFacebook)\n",
    "df = df.T\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFairness = fixCommunityFairness(facebookLargestCCGraph, genderCountFacebook, facebookLowDegreeNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFairness\n",
    "dfNew = pd.DataFrame(newFairness)\n",
    "dfNew = dfNew.T\n",
    "dfNew.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_communities_facebook = nx.community.louvain_communities(\n",
    "    facebookLargestCCGraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community_counter = 0\n",
    "# with open(\"output/facebookLouvain.txt\", \"w\") as f:\n",
    "#     for community in louvain_communities_facebook:\n",
    "#         for node in community:\n",
    "#             f.write(\"{}\\t{}\\n\".format(node, community_counter))\n",
    "#         community_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read comE outputs communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communitiesPathList = os.listdir(\"comE_communities\\communities\")\n",
    "# communitiesPathList\n",
    "convert_Communities_To_Fairness_CSV(communitiesPathList)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2a152c9c8174e8f19b1600c749953c43c503b31f710d00df083fd6ee6632082"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
